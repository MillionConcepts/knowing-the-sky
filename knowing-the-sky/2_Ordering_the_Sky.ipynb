{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45489af",
   "metadata": {},
   "source": [
    "## Lesson 2: Ordering the Sky\n",
    "#### Learning Objectives:\n",
    "After completing this lesson, users will be able to:\n",
    "1. Describe array and table data types and their uses\n",
    "2. Utilize python packages to open array and table data as represented by the data providers\n",
    "3. Index and manipulate data within arrays and tables\n",
    "4. Load and manipulate astronomical data into formats compatible with standard Python libraries\n",
    "\n",
    "_Python libraries introduced in this lesson:_ [`astropy.io.fits`](https://docs.astropy.org/en/stable/io/fits/index.html), [`matplotlib`](https://matplotlib.org/stable/), [`numpy`](https://numpy.org/doc/stable/), [`pandas`](https://pandas.pydata.org/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986821e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# third-party imports\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c1aaf-7fcc-417f-8a15-45bdf6e7c17b",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In our last lesson, we aquired data from online NASA repositories to begin an investigation to identify which stars were being referred to by the Salish individual(s) interviewed by Claude Schaffer:\n",
    "\n",
    ">There was a group of three stars that rose from the place that was between the sun and the moon and it never changed its positon of rising.\n",
    "\n",
    "In this lesson, we'll learn how to read that data into python standard objects and the basics of how to work with them. The two main types of data we'll cover in this lesson are arrays and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9f81c",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Arrays are one of the most important data structures in scientific programming, and, for that matter, practical mathematics more generally -- they are a basic pattern used in many systems and cultures. Computers are very good at working with them, so knowing how to manipulate them effectively is extremely powerful. If you are reading this, it is entirely possible you have extensive experience with array data. Things may be different in Python than other languages, though, so we recommend not skipping it.\n",
    "\n",
    "#### What is an array?\n",
    "\n",
    "Basically, an array is simply a regular grid of 'elements', which are often but not always numbers. It can be 1-D, 2-D, 3-D, or even more. A 1-D array is a lot like a list. \n",
    "\n",
    "You can access elements in an array by coordinates, which means that they work like coordinate spaces. This is *very* useful for scientific programming. You can also perform mathematical operations on an entire array at once, which is both convenient and typically much faster than performing the same operations on each element one-by-one.\n",
    "\n",
    "(footnote for the mathematically inclined): an array is *not* a vector, matrix, or tensor, but it can be used to *represent* a vector, matrix, or tensor. \n",
    "\n",
    "#### How do you work with arrays in Python?\n",
    "\n",
    "`numpy` is the most common library for creating and manipulating arrays. Most scientific programs use `numpy` at some point, partly because many other libraries use it 'under the hood'. A very important point is that raster images (images made up of pixels, as opposed to vector images) can always be converted to and from arrays, which means that arrays let you do very powerful image manipulation. Let's start with a very quick introduction to the most important parts of the `numpy` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a1dd1-e36d-4187-8d40-63a1a3c63055",
   "metadata": {},
   "source": [
    "First, let's make a little 4x4 triangular array we can play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle = np.tri(4)\n",
    "triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b437-3d14-45f9-a945-9e47b29cfc90",
   "metadata": {},
   "source": [
    "You can access single elements of an array by entering their coordinates in slice `[]` notation -- much \n",
    "like a Python list but with possible extra dimensions.\n",
    "\n",
    "A reminder that python is \"zero-indexed\" meaning when you are counting elements in an array, list, etc. You start from 0 not 1. (This is one of the major differences between Python and MatLab!)\n",
    "\n",
    "For a 2-D array, the row (y-axis) always goes first, then the column (x-axis). You get exactly the elements you specify, which means that yif you don't enter a number for every dimension, you can pick more than one element at a time. For instance, `[0]` means \"all the elements with 0-coordinate on the y-axis\", or, in other words, the first row. `[:,0]` (`:` is numpy's placeholder) means \"all the elements with 0-coordinate on the x-axis\", and returns the first column. If you specify a number for every dimension you can get just a single elements: `[0,0]` means \"the number\n",
    "in the upper-left corner\".\n",
    "\n",
    "Let's test this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feda94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle[0], triangle[:, 0], triangle[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910e0d1-56ea-4863-8162-27e299448d0b",
   "metadata": {},
   "source": [
    "You can also specify ranges. For instance, `[0:2, 0:2]` means \"give me the square in the upper left corner of size (2,2)\". \n",
    "\n",
    "Note here that the range is exclusive of the final index you're entering. In other words, you'll get rows/columns indexed 0-1, not data from row/column 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle[0:2, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccec8e0-4621-4aa9-85b3-5f8535f14fe9",
   "metadata": {},
   "source": [
    "When working with arrays you read in from data, you won't always know what shape it is because you won't have made it like we did above. In numpy, `shape` can be used to return how many elements an array has along each of its dimensions. It's important to know an array's shape for a variety of reasons, but on eo fhtem is to know exactly how much there is to slice. Let's return our array's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56267f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ff563-a501-4565-8f2d-5dafce56d87d",
   "metadata": {},
   "source": [
    "Remember that Python is 0-indexed, and that `.shape` returns the _number of elements_, not the index. That means this will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6ddaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "triangle[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1431915-7cbb-46da-89a9-57c896b0af6c",
   "metadata": {},
   "source": [
    "Slice notation can also be used to set elements. If you do the same things we've been doing, but add an assignment (`=`), you can set all the values to a number -- or even set to the contents of another array, if it's the right size.\n",
    "\n",
    "This will set all the elements of the first row to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a590a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle[0] = 2\n",
    "triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e23eb-1349-4899-837c-b1ab1e911315",
   "metadata": {},
   "source": [
    "If you do an arithmetic operation on an array with a scalar (like a single number), it will apply that operation to ever element of the array. Unlike slice assignment, this does not modify the array in place unless you use a special operator. It retuns a copy, leaving the original array unchanged. \n",
    "\n",
    "For instance, to get a copy of the array with every element doubled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "double = triangle * 2\n",
    "double"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12b2e8-efab-4f37-99f2-f5ee79d5e978",
   "metadata": {},
   "source": [
    "Doing arithmetic using two array perfoms the operation elementwise -- elements with matching indexes from each array. Let's create a random array to test this with. We'll use [`np.random.randint`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) to create an array of random whole numbers between 0-10 the same shape as triangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5207c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "randarray = np.random.randint(0,10,size=triangle.shape)\n",
    "randarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed96b87-45ce-4ae9-a223-f50b7f3daa7d",
   "metadata": {},
   "source": [
    "and then multipy it by triangle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37006132",
   "metadata": {},
   "outputs": [],
   "source": [
    "randarray * triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d71fd-5c1f-4e97-a181-2c844908cc96",
   "metadata": {},
   "source": [
    "This doesn't work if you try to multiply two arrays with mis-matched shapes! Let's test this by trying to multipy a 3 x 3 array of 3 by triangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42ca7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "three = np.full((3, 3), 3)\n",
    "three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7725d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "three * triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82109cf3-0907-4871-b95e-7d002db65842",
   "metadata": {},
   "source": [
    "As expected, we get an error. But if we slice our larger array to be the same shape as our 3x3 array we can multiply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "three * triangle[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df10904-dd94-476f-b051-da9f306f59e9",
   "metadata": {},
   "source": [
    "You can even do assignment like this! This will multiply the upper-left 3x3 square of `triangle` by `three`, in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026df528",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle[0:3, 0:3] = three * triangle[0:3, 0:3]\n",
    "triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff44790-2c4f-4647-af37-9086d781a130",
   "metadata": {},
   "source": [
    "Finally, you can also slice an array _with_ another array. This can get complicated fast: `numpy` calls it \"fancy indexing\" for a reason. However, there are a lot of very simple ways to use it. This most common one is to just select all the elements of an array tha tmeet some condition. For instance, this means \"pick all the elements of triangle that are less than 3\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfc4a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triangle[triangle < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dca87d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Loading images\n",
    "\n",
    "Ok, intro examples over; let's take a look at something more practical. We'll load a FITS image into memory using `astropy.io.fits` and look at some basic ways to use it.\n",
    "\n",
    "FITS files are the most popular standard for astronomy data and are becoming increasingly popular in planetary science data. They consist of multiple header-data units or HDUs. Each HDU includes a header area containing metadata and a data area containing, unsurprisingly, data. This data can be an array or a table. In  this case, all the HDUs are arrays. `astropy.io.fits` loads FITS file into an `HDUList` object, whichin most cases can simply be treated as a Python list with  some extra useful methods, like `.info()`.\n",
    "\n",
    "_Remember_: at the beginning of this notebook we imported `astropy.io.fits` as `fits` so we can call it like that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfe5bc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: placeholder image\n",
    "\n",
    "hdul = fits.open('placeholder_images/e23456-nd-t0060-b00-f0008-r.fits')\n",
    "hdul.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7dea2-4565-4196-8f5d-2c42253d5b70",
   "metadata": {},
   "source": [
    "As you can see from the () dimensions entry, HDU 0 contains no actual data -- it's a placeholder to help organize the rest of the file. HDU 1, however, contains an actual array. Let's go ahead and get the data from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu = hdul[1]\n",
    "image = hdu.data\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612f223",
   "metadata": {},
   "source": [
    "### Exploring arrays\n",
    "\n",
    "As we saw earlier, the representation `numpy` provides for arrays is excellent for small arrays. Unfortunately, as we can see here, for large arrays, it doesn't really tell you much except the data type **todo: should we have a section on data types?** and what's in the corners. Fortunately, Python offers many tools for summarizing and visualizing arrays. \n",
    "\n",
    "#### Dealing with nonfinite values\n",
    "\n",
    "Before we start exploring, we need to deal with the fact that lots of scientific data -- including this image -- contain 'nonfinite' values: `nan` (not-a-number, sometimes called 'null', used for missing or invalid entries), `inf` (infinity), and `-inf` (negative infinity). This means many statistical tools will fail on them out of the box: what, after all, is the mean of 2, 3, and not-a-number? There are several ways to work with data despite nonfinite values. The most powerful tool `numpy` provides is the masked array, which allows you to ignore specific elements and work with the array just like you would otherwise. A caveat: not all libraries 'respect' masked arrays, so you may have to do fancier tricks when working with some tools. However, many do. We'll show a common trick for dealing with libraries that don't a little further down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6ed4d-61b6-4719-a538-e0d626f2aec8",
   "metadata": {},
   "source": [
    "Here you'll see due to the `nan` values `np.mean` does not provide a useful result on the original array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56b5d7-6dcb-4ccf-b4b1-f90ea97a6029",
   "metadata": {},
   "source": [
    "However, if we construct an array that masks all nonfinite elements, we're in business:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.ma.masked_invalid(image)\n",
    "np.mean(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb704e64-6636-4120-be9b-d93a2430962c",
   "metadata": {},
   "source": [
    "Let's go ahead and also mask everything below 0 (0 is valid minimum for this image). The mask is just an array of boolean (True/False) values, and you can slice an assign it just like any other array. We'll use the fancy indexing trick from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22616e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.mask[image < 0] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26dcb8",
   "metadata": {},
   "source": [
    "#### Summarizing arrays\n",
    "\n",
    "`numpy` offers many built-in tools that are good for simply describing arrays as well as performing mathematical operations. Commonly-useful ones include `std` (standard deviation), `mean`, and `median`. We just looked at `mean`. Let's look at the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a25527",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    np.std(image), \n",
    "    # using the version from the np.ma namespace because of a little glitch in core numpy\n",
    "    np.ma.median(image)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e1da1-ac28-431f-a552-0a6b6b29bbb0",
   "metadata": {},
   "source": [
    "These can also be used along an individual axis. for example, if you'd like the mean of every column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d750f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca67d85",
   "metadata": {},
   "source": [
    "#### Outside of numpy\n",
    "\n",
    "For really serious statistical work, it's often useful to turn to `scipy.stats`, which has a wide range of useful tools. `scipy.describe` is great for getting a bunch of basic descriptive statistics at once, *usually*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedddf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.describe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8b356-f28f-41ac-810f-055509b834e9",
   "metadata": {},
   "source": [
    "Hmmm...that's a lot of nans, and it worked along only one axis, which probably isn't what \n",
    "we wanted. here's a little trick for getting just the unmasked elements of an array, \n",
    "'raveled' into a single dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = image[~image.mask]\n",
    "stats.describe(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba4c25",
   "metadata": {},
   "source": [
    "### Visualizing arrays\n",
    "\n",
    "Since many arrays are images, we often want to simply *look* at them. This, of course, can even be useful for arrays that _aren't_ exactly images. There are many ways to do this, but the `imshow` function from `matplotlib.pyplot` is one of the most straightforward: it simply plots a 1D, 2D, or 3D array on a grid. Let's use it to take a look at the image we just opened -- although the results may not be satisfying at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cb4f7-69b7-4c27-bc5a-44c0c743cb53",
   "metadata": {},
   "source": [
    "Unfortunately, plt.imshow, by default, applies a linear 'stretch' to the image. Because the range\n",
    "of values in this image are so wide, it doesn't look like much at all -- almost everything is at the\n",
    "lower extreme of the color scale.\n",
    "Let's write a little function that uses numpy to range-clip the image so we can look at it more clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_clip(array, sigma=1):\n",
    "    # find the mean and standard deviation of the array\n",
    "    mean, std = np.mean(array), np.std(array)\n",
    "    # restrict the bounds of the array to (mean - sigma * std, mean + sigma * std)\n",
    "    return np.clip(array, mean - sigma * std, mean + sigma * std)\n",
    "\n",
    "clipped = std_clip(image)\n",
    "plt.imshow(clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbdf63-68e5-4a9e-9680-fe9192303acd",
   "metadata": {},
   "source": [
    "Because we made the `sigma` variable as part of the function input, we can modify the stretch however we like. If we'd like a brighter (but noisier) image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_clipped = std_clip(image, sigma=0.25)\n",
    "plt.imshow(really_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984d0e6-3169-4291-b227-aa12b72940fb",
   "metadata": {},
   "source": [
    "Slicing works very well on arrays considered as images. For instance, you can use it to take a 'cutout' from an image to look at a portion in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(clipped[1000:1200, 1000:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cd26b",
   "metadata": {},
   "source": [
    "## Tables\n",
    "Tables are basically arrays.\n",
    "\n",
    "### Ok, but what is a table, really?\n",
    "A table is _like_ an array, with the following differences:\n",
    "* 'Table' implies 2-D: it has rows and columns and nothing else.\n",
    "* The columns of a table are also called 'fields', and have\n",
    "    *meanings* that are distinct from one another: for instance, a table created from a radiometer \n",
    "    on a satellite might have fields representing time, latitude, longitude, and brightness temperature;\n",
    "    individual rows would represent individual observations.\n",
    "    * This is different from arrays, where, in general, every element has a roughly equivalent *meaning*, but in a different *place*.\n",
    "* A table can have more than one data type: for instance, a star catalog might have a field representing\n",
    "    right ascension in 64-bit floating-point numbers, and a field representing star names as strings.\n",
    "* **todo: maybe not necessary here** A table also generally has an 'index', which can be \n",
    "    a numerical series that simply corresponds to row number, but can be something else\n",
    "    entirely -- for instance, a table of personnel data might be indexed by employee ID number.\n",
    "\n",
    "Like arrays, tables are of ancient origin and found in many forms in many contexts, but computers really like to work with them. Tables are even more widely used than arrays in general-purpose computing. Common examples include spreadsheets and SQL databases. The world runs on tables.\n",
    "\n",
    "### Tabular file formats\n",
    "\n",
    "#### Broad categories: binary vs. text\n",
    "Tables can be stored in many formats. The two major _categories_ of format are binary and text tables. Text table formats store values as human-readable strings, like:\n",
    "\n",
    "| RA | DEC |\n",
    "| ------ | ----- |\n",
    "| 17.222 | -12.301 |\n",
    "| 17.223 | -12.311 |\n",
    "| 17.222 | -12.323 |\n",
    "\n",
    "These sorts of tables are relatively easy to look at and manipulate in spreadsheet programs or text editors -- unless they get so big that the program will refuse to load them!\n",
    "\n",
    "By contrast, binary table formats store values as raw bytes. The number of bytes per entry depends on the underlying data type. They are poorly human-readable, but tend to use less disk space and be quicker to load than text tables. As 32-bit floating point numbers expressed in bytes, the preceding table might look like:\n",
    "| | |\n",
    "|-|-|\n",
    "| \\xa8\\xc6\\x89 | \\xe5\\xd0D\\xc1 |\n",
    "| \\xb4\\xc8\\x89A | \\xe5\\xd0D\\xc1 |\n",
    "| \\xa8\\xc6\\x89 | \\x02+E\\xc1 |\n",
    "\n",
    "This is about 50% smaller in terms of data volume than the text version, but most people are not going to get much out of just _looking_ at it, and trying to manipulate it by copy-pasting would be fruitless. To use it, you need to load it into software that will translate it into meaningful numbers.\n",
    "\n",
    "Some binary table formats simply contain encoded values. Other formats embed metadata along with the values (for instance, cell colors in an Excel file). These formats tend to be harder to read.\n",
    "\n",
    "The upshot of all this is that really large tables -- tables that you wouldn't be able to visually scan, or maybe even successfully load, in a spreadsheet or text editor -- should almost always be stored in binary formats. Small tables are fine to store as text. If you're only planning to use them as intermediate data, though, small tables might also just as well be binary.\n",
    "\n",
    "#### Characteristics of some specific formats\n",
    "There are many, many tabular data formats. This is not an exhaustive list, but here are some common formats you are likely to encounter in planetary science and astronomy:\n",
    "\n",
    "**DSV (delimiter-separated value)**\n",
    "\n",
    "DSV is a family of text table formats that place each row on a separate line, and separates columns with a special delimiter character, usually a comma or a tab (a tab is represented by '\\t' in most programming languages). Comma-separated tables are often called CSV, and tab-separated TSV. This is a CSV table:\n",
    "```\n",
    "satellite,1.002,-379,True\n",
    "unknown,1.547,-22,False\n",
    "```\n",
    "Common file extensions for DSV files are .csv, .tsv, .txt, and .tab, but there are many others.\n",
    "\n",
    "\n",
    "**FWF (fixed-width file)**\n",
    "\n",
    "FWF is a text table format that places each row on a separate line, and separates columns by defining a specific 'width' -- number of characters -- for each column. These columns may also have whitespace 'padding' to make them easier to read (and for programs to figure out how to parse them if they don't have the definition available). FWF files were traditionally easier for programs to read than DSV files, but this is no longer true. In general, we recommend choosing DSV formats over FWF, because the delimiters make it easier for a wider variety of programs to parse them, they are usually a little smaller (because not every row has to be the same width), and you are less likely to make mistakes if you edit them manually. This is a fixed-width version of the DSV table above:\n",
    "```\n",
    "satellite 1.002 -379 True \n",
    "unknown   1.547 -22  False\n",
    "```\n",
    ".tab and .txt are common file extensions for FWF, but, like DSV, providers often exercise creativity.\n",
    "\n",
    "\n",
    "**Excel (and other spreadsheet formats)**\n",
    "\n",
    "Spreadsheet files are generally binary tables with embedded metadata that allow spreadsheet programs to retain formatting like fonts, cell colors, and sheet breaks. These are usually *proprietary* formats: although they can be reverse-engineered, the makers of the software do not publish specifications for them, and they are subject to change between software versions. This means that they are poorly portable and not very future-proof. For these reasons, we do not recommend exchanging scientific data in spreadsheet formats, and most spreadsheet software can easily export data in standard formats like CSV. Most spreadsheet formats have extensions that indicate the specific software used to produce them: for instance, Excel uses .xlsx or .xls.\n",
    "\n",
    "**FITS**\n",
    "\n",
    "FITS files, which were invented for astronomy but have seen widespread adoption in other scientific disciplines, can contain either binary or text tables (although FITS text tables are rarely used in practice). Because FITS files, as we discussed earlier, can have multiple HDUs, a single FITS file can contain multiple tables. FITS files usually have a .fits or .fit extension. We think FITS is one of the best ways to save tables due to its widespread support and mature, stable standard.\n",
    "\n",
    "**PDS binary tables**\n",
    "\n",
    "Many tabular data products in the PDS have ad-hoc structures they describe in external metadata. Standards differ between PDS3 and PDS4. In PDS3, structures can be defined in either their attached or detached PDS3 labels, or in an arbitrary number of distinct format (.fmt) files. In PDS4, binary table structure must be completely defined in a file's detached XML (.xml) label. Common extensions for these files include .dat and .tab.\n",
    "\n",
    "**SQL databases**\n",
    "\n",
    "**TODO: maybe skip this?**\n",
    "\n",
    "**Parquet**\n",
    "\n",
    "Parquet is a newer tabular interchange format that has seen wide adoption in industry and is increasingly common in scientific contexts. It is a _columnar_ format, which means that every field is stored in a distinct area and can be both compressed and readily accessed individually, which makes it very efficient for many purposes. We think Parquet is also an excellent choice for tabular data, especially very large tables that you don't want to load into memory all at once.\n",
    "\n",
    "**general-purpose data formats**\n",
    "\n",
    "Finally, while this category is much to broad to detail, it's worth noting that tables can also be represented in structured data formats that aren't specialized for tables, like Javascript Object Notation (JSON) and Extended Markup Language (XML). (In fact, Excel files rely in part on a compressed and modified version of XML!) Features of programming languages that save in-memory objects to disk, like Python `pickle` files or MATLAB .mat files, can similarly be used to store tables -- but then they can only be opened in that programming language! We don't recommend doing this unless you have a really good reason to.\n",
    "\n",
    "**TODO: maybe something about headers?**\n",
    "\n",
    "\n",
    "### Working with tables in Python\n",
    "\n",
    "There are many packages for working with tabular data in Python. `pandas` is by far the most common, and it's what we'll mostly use in this book. `pandas` has built-in support for reading and writing a wide variety of table formats, but not all, so you'll sometimes need to use a helper package to load data into `pandas`. For instance, `pandas` can't read FITS tables, so you'll need to go through a specialty FITS library like `astropy.io.fits` or `fitsio`. Similarly, to read tables described in PDS metadata, you'll need to use a library like `pdr` or `pds4-tools`.\n",
    "\n",
    "Also, as you saw in the previous lesson, people often compress table files 'monolithically', wrapping the whole file at once in a compression format like gzip. Some packages can decompress files like this automatically, but most can't, so you'll often need to decompress table files before you can use them.\n",
    "\n",
    "`pandas` is very, very powerful; however, it has many features that don't follow a common idiom, so it needs to be used with care. Let's move on to some basic ways to use `pandas`. We'll use the Bright Star Catalog as our sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a2f46-893b-4ce5-8788-fec3022f157f",
   "metadata": {},
   "source": [
    "The BSC 'catalog' file has no file extension. However, if you open it in a text editor like Notepad or TextEdit (go ahead and do that now), you'd quickly see that:\n",
    "\n",
    "1.  it has a text table, and\n",
    "2.  it has no delimiter characters. (characters that show where each piece of data ends; like commas in a .csv file)\n",
    "\n",
    "This means that the BSC catalog is a fixed-width file. (Each cell of table is an equal number of bytes)\n",
    "You'd also notice that a lot of the fields appear to run togehter, which meake it somewhat hard to read -- both for humans and for `pandas`. Let's see what happns if we just use the `pandas` `read_fwf` function, which attempts to read a FWFby inferring the width of each field. The `header=None` argument tells `pandas` that the table has no column headers (data in the file before the start of the table that usually provides metadata or other information).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11bcca-0db1-4a2e-98e2-a2eb885e78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_take_1 = pd.read_fwf('bright_star_directory/catalog', header=None)\n",
    "bsc_take_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc807797-f5f1-4464-8b2c-f1727cb5002f",
   "metadata": {},
   "source": [
    "Ok, that looks _sort of_ reasonable. We can validate it by opening the specification of the table in bright_star_catalog/ReadMe in a text editor...\n",
    "\n",
    "(Do that now so you can follow along and get experience looking at these kinds of documents. This kind of check is your main line of defense against using data you _think_ was read in properly but actually wasn't!)\n",
    "\n",
    "And, unfortunately, if you scroll down to line 77, \"Byte-by-byte Description of file\", you'll see that it's not correct. The specification gives over 50 columns, while pandas detected only 14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_take_1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86306f-a544-4f74-be6c-3781200b729e",
   "metadata": {},
   "source": [
    "So, we have a couple of options here. If you take a look at the read_fwf() documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html), you'll see that you can pass a list of column specifications to read_fwf() in its colspecs argument.\n",
    "However, we'd have to type all of those in, which is doable, but doesn't sound like much fun.\n",
    "\n",
    "The easy direct download we used last time deceived us. So let's, instead, try to get this into `pandas` another way. [CNRS](https://www.cnrs.fr/en)'s Vizier service offers access to many complete catalogs. Let's see if it has the BSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.vizier import Vizier\n",
    "\n",
    "catalogs = Vizier.find_catalogs('Bright Star Catalogue')\n",
    "len(catalogs.keys()) # print the number of matching results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c562805-3e31-40ac-a10e-15ab92fba425",
   "metadata": {},
   "source": [
    "`Vizier.find_catalogs()` returns an `OrderedDict`. Each key in the dictionary will be a different catalog name. We used `len(catalogs.keys())` to print the length of the `keys()` attribute of the `OrderedDict` to know how many catalogs matched our search. It's got 19 catalogs related to 'Bright Star Catalogue'! Let's take a look at which specific one we want based on their descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    c: r.description for c, r in catalogs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297f1ba-150d-417e-a71d-e346f88ad953",
   "metadata": {},
   "source": [
    "It looks like 'V/50' is our target. Let's go ahead and download the whole thing from Vizier.\n",
    "by default, Vizier will only return 50 rows, and we want the whole table, so let's \n",
    "make a new Vizier object with changed settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_vizier_fetcher = Vizier(row_limit=99999)\n",
    "tables = big_vizier_fetcher.get_catalogs([catalogs['V/50']])\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23feaa46-caaa-4181-9883-7e054e30ce10",
   "metadata": {},
   "source": [
    "Now we've got the catalog _and_ the notes in `tables`. Let's look at the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ae3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaef46-391c-4e2f-845c-795b13bedd6e",
   "metadata": {},
   "source": [
    "This doesn't have quite as many columns as the 'full' version, but they're separated correctly,\n",
    "and we don't need most of that metadata. \n",
    "you'll also note that `astroquery.vizier` returned this as an `astropy Table` object, but we'd rather\n",
    "work with it in `pandas` so that works better with other python libraries we might use. `astropy Tables` can be easily converted to `pandas`, so let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc44265",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc = tables[0].to_pandas()\n",
    "bsc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a56573-eaa1-4789-9449-96f89fe99fae",
   "metadata": {},
   "source": [
    "That's more like it! now, to prevent us from having to fetch it again,\n",
    "Let's write it out in a format pandas will be able to understand more easily --\n",
    "a simple CSV file.\n",
    "\n",
    "*Note*: the `index=None` argument means \"don't write the index as a separate column\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929384af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc.to_csv('bright_star_directory/catalog.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c414265-821a-46bd-887a-831092825a03",
   "metadata": {},
   "source": [
    "Now, let's verify that we can read it in again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49362c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in = pd.read_csv('bright_star_directory/catalog.csv')\n",
    "bsc_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fd959-3921-4e74-a749-506659b1a145",
   "metadata": {},
   "source": [
    "Great! It looks the same, with one notable exception: it reinterpreted some\n",
    "of the columns as numbers, which also caused it to fill in blank spaces with NaNs. \n",
    "This is, in fact, a very good thing! For instance, if we'd wanted to look at all the\n",
    "stars with Double Star Catalog designation (ADS) < 50, and had tried to do that on the \n",
    "original table, we would have gotten an error, because Vizier specified that column\n",
    "as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2ed1-d31a-4026-88c4-216042ee446f",
   "metadata": {},
   "source": [
    "Okay, so what can we do with this object?\n",
    "\n",
    "First, a little vocab. This object is called a **DataFrame**.\n",
    "\n",
    "This is one of the two basic `pandas` types. The other basic `pandas` type is a **Series**. Every field/column in a DataFrame is a Series. You can get the type of a python object with the `type()` function. Let's try that here to confirm what we've just learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bsc_in), type(bsc_in['RAJ2000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1a7d7-d27a-41d3-a474-d7aec8f90f23",
   "metadata": {},
   "source": [
    "Like `numpy`, `pandas` has a lot of tools for accessing elements of a table, but it works differently, principally because rows and columns can have names, not just numbers.\n",
    "\n",
    "`pandas`' core tools for this are called **indexers**, and it has three of them:\n",
    "`.at`, `.loc`, and `.iloc.`.\n",
    "\n",
    "`.at` is rarely used, so we won't focus on it here. You can read more about it [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) in the pandas docs.\n",
    "\n",
    "`.loc` fetches one or more elements by name: index name(s) first, column name(s) second. Like `numpy`, `pandas` uses `:` as a placeholder, and you don't have to give it both index and column. Let's look at some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2a6a2-7660-4cdc-8743-fdd866432c05",
   "metadata": {},
   "source": [
    "This is how you would get the visual magnitude (Vmag) of stars 3000-3010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5329898",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[3000:3010, 'Vmag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f978a35-8393-4f7d-841d-34dd16637a0a",
   "metadata": {},
   "source": [
    "You might be thinking: Well how did you know to type `'Vmag'` in the example above? In the examples where we printed the table itself we saw the column names. So you could have gotten them from them there, but what if you don't want to print the entire table? You could get a list of the column names using `.columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc1b68-ea30-4df1-a03a-f0698e9b5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad147c4c-497d-40f2-a2bf-da8a1def7313",
   "metadata": {},
   "source": [
    "Back to our `.loc` examples, this is how you would get everything in the 2001th row: (!Remember! the first row is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ea3ca-9bdd-4433-8d17-3b1af7e3631f",
   "metadata": {},
   "source": [
    "This would return the entire Vmag column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[:, 'Vmag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1da5d4-076b-413d-bc31-c7e8950a30c8",
   "metadata": {},
   "source": [
    "If you simply use slice notation right on the dataframe, this will work the same as using `.loc[:, column_name]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in['Vmag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66563492-10ed-4edb-ae24-ac5ebc8fcd4e",
   "metadata": {},
   "source": [
    "You'll note that these last few gave `Series` objects rather than `DataFrame` objects. (Again if you'd like to test that just use `type()`). This is becuase we only got a single row or column at once. It's possible to slice a dataframe out of another dataframe. Because the columns have string names rather than numbers, you can do this by passing a list. Here is the visial magnitude and right ascension of stars 30-40, presented as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d25ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[30:40, ['Vmag', 'RAJ2000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6954-9054-4e8f-a204-15745563b8ed",
   "metadata": {},
   "source": [
    "Like `numpy` arrays, you can select elements of a DataFrame that meet some special condition. The easiest way to do that is by using an expression that evaluates to either true or false. An expression like this is called a **logical predicate** and this one will produce a [boolean](https://www.w3schools.com/python/python_booleans.asp) Series giving us only very bright star with Vmag < 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9236b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_condition = bsc_in['Vmag'] < 2\n",
    "bright_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e08eff-f6e4-4d03-93a4-1519e18241ab",
   "metadata": {},
   "source": [
    "We can then pass that Series to `.loc` in order to select all the rows of the DataFrame for which that condition is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbda8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright = bsc_in.loc[bright_condition]\n",
    "bright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb9dd0-fe46-49b3-b70d-bb97b9be263d",
   "metadata": {},
   "source": [
    "Like arrays, the values in DataFrames can be easily plotted with matplotlib.\n",
    "For instance, to make a scatter plot of Vmag vs. color (B-V) the following expression could be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed104492",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bsc_in['Vmag'], bsc_in['B-V'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0f711-1e2b-430b-8fe8-358e1b150b4d",
   "metadata": {},
   "source": [
    "Like arrays, you can assign values to particular elements of a DataFrame,\n",
    "and you can do operations on big swaths of a DataFrame at once.\n",
    "\n",
    "If we look at one of the values for in the 'Name' field, you'll note that there's some uncessary padding (extra spaces):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740838e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[2, 'Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bb55d-9767-442e-972f-7700ffea19ec",
   "metadata": {},
   "source": [
    "Here we can use the `.str` method of the `pandas Series` to treat each value in the 'Name' field as a string, and use `.replace` to switch out the sequences that match the regex pattern `' +'`. That plus sign tells the code to look for any sequence of more than one of the character it comes after, in this instance, a space. Of course, we use `regex=True` to tell `.replace` we are using a regex pattern otherwise it will look for the literal characters space then plus sign in the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in['Name'] = bsc_in['Name'].str.replace('  +', ' ', regex=True)\n",
    "bsc_in.loc[2, 'Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f8abf-14bc-4fd1-9833-c6250b741cf3",
   "metadata": {},
   "source": [
    "Unlike arrays, you can add new columns as you please. Vmag is logarithmic in base ~2.512, so \n",
    "if you wanted a linear version of it for some calculation, you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in['Vmag_exp'] = 2.512 ** bsc_in['Vmag']\n",
    "bsc_in['Vmag_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db9d17-5933-4aca-bc5e-530878111a7f",
   "metadata": {},
   "source": [
    "We don't really want to keep that though, so let's take a look at how we can easily drop that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58be093",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in = bsc_in.drop(columns='Vmag_exp')\n",
    "bsc_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bc555-03dd-46c9-8aad-05a9a627f35b",
   "metadata": {},
   "source": [
    "You can also drop rows. There are a few entries in the BSC that don't have actual\n",
    "coordinates. Let's clean them up quickly. Note that the '~' means 'not', so we're \n",
    "saying: \"get rid of all the rows that don't have a real RAJ2000 value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in = bsc_in.loc[~(bsc_in['RAJ2000'] == '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2adb74-b547-425c-b0dd-0ea8fa81738f",
   "metadata": {},
   "source": [
    "After you do this sort of thing, it's usually a good idea to copy the dataframe and reset the\n",
    "index -- otherwise you can get confusing errors. (There are cases where this isn't\n",
    "true, of course, but it's a good rule of thumb.) This is the easiest way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f5ef0-f1de-4061-9bc4-9c549fb4fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in = bsc_in.copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3860d6-7726-44f7-81ad-f13550d64947",
   "metadata": {},
   "source": [
    "There's one more thing we should do before we move on. The BSC gives RA and DEC\n",
    "in hours/minutes/seconds notation. This is all well and good, but can be hard for general-purpose\n",
    "tools to work with, because they don't usually understand this notation. Hence this error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf95862",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[(bsc_in['RAJ2000'] < 20) & (bsc_in['DEJ2000'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d8e22-10e4-40a8-a901-5c5ab3c054a8",
   "metadata": {},
   "source": [
    "Fortunately, `astropy` _does_ understand this notation, and we can use it to convert these\n",
    "from hours/minutes/seconds to decimal. The `Angle` object from `astropy.coordinates` is our helper here.\n",
    "Note that you have to specify the units, or astropy will be unhappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Angle\n",
    "ra0 = Angle(bsc_in.loc[0, 'RAJ2000'], unit='hour')\n",
    "# astropy parses and prints it nicely:\n",
    "ra0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039df236-ecb5-4d83-b48e-152bec57c1a3",
   "metadata": {},
   "source": [
    "and then you can convert it from hour-minute-second to decimal degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra0_decimal = ra0.degree\n",
    "ra0_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009343f8-2425-4bde-a5e1-f65977a24992",
   "metadata": {},
   "source": [
    "That's all well and good for a single value -- but you can even pass the entire\n",
    "Series at once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_angles = Angle(bsc_in['RAJ2000'], unit='hour')\n",
    "ra_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16570a9-2577-4488-a7ea-6c5c3897f17a",
   "metadata": {},
   "source": [
    "And convert it all to decimal at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_decimal = ra_angles.degree\n",
    "ra_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b5b82-f6ab-4f26-a3c6-32b649f8f855",
   "metadata": {},
   "source": [
    "Let's do the same thing with declination, which is in degree-minute-second\n",
    "rather than hour-minute-second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_angle = Angle(bsc_in['DEJ2000'], unit='deg')\n",
    "dec_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_decimal = dec_angle.degree\n",
    "dec_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf7058-6339-4a2b-9289-b0dd7e8ee418",
   "metadata": {},
   "source": [
    "Now we can go ahead and replace the ra/dec strings in the dataframe\n",
    "with our decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in['RAJ2000'] = ra_decimal\n",
    "bsc_in['DEJ2000'] = dec_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868db33-a221-4844-95b7-a9c927de7f7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And you'll find that our earlier expression will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe97c1-c263-4f6e-ade1-e1fa14528919",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.loc[(bsc_in['RAJ2000'] < 20) & (bsc_in['DEJ2000'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0b05e-e53c-4dab-9bea-38532b95d601",
   "metadata": {},
   "source": [
    "Now we can write our nice-cleaned up table to a new CSV file and use it in the next \n",
    "Notebook without worrying about all of these fiddly details again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312fd49-d470-4e7d-b425-8cf312997937",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_in.to_csv(\"bright_star_directory/bsc_clean.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
