{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446fef1e-23c0-482b-add2-e5a235d07f4b",
   "metadata": {},
   "source": [
    "## Introduction to Low-Level Image Processing\n",
    "\n",
    "You can get a long way in image processing and computer vision with\n",
    "fairly simple operations that basically just work with arithmetic.\n",
    "They're not only easier to use than more complex algorithms, but \n",
    "also usually faster, easier to inspect, and more reliable than \n",
    "complex algorithms. \n",
    "\n",
    "It's good to understand low-level techniques if you think you might want \n",
    "to use more complex techniques for image recognition or processing, \n",
    "including machine learning methods, because complex techniques often \n",
    "don't work well without preprocessing by simpler functions. Simple \n",
    "functions can also be assembled into quite complex ones!\n",
    "\n",
    "In those notebook, we'll primarily be using \"morphological\" operations \n",
    "-- operations that work with line, shape, and form. However, we'll start\n",
    "with a little introduction to their extended family.\n",
    "\n",
    "### Footprint Operations\n",
    "\n",
    "Many image manipulation techniques rely on looking at each pixel in the \n",
    "image, then applying some kind of mathematical function to the pixels \n",
    "that fall within some region around it. This region is called a \"footprint\"\n",
    "(or sometimes a \"kernel\").  \n",
    "\n",
    "Let's look briefly at a useful _non_-morphological example of this\n",
    "technique: the median filter. This filter works by replacing each pixel \n",
    "in an image with the median value of the pixels that fall within a footprint\n",
    "around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b075d-d059-42d5-b9c4-8e4b999f5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# NOTE: I don't remember if we opened 'desktop' images before.\n",
    "from PIL import Image\n",
    "import scipy.ndimage as ndi\n",
    "# This is a little settings file that removes all the annoying\n",
    "# borders and axes and things that aren't really relevant\n",
    "# when we're just using matplotlib to look at things as images.\n",
    "plt.style.use('settings/simple_image.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587859cb-6122-4141-9944-47fbc72b8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a noisy cross shape.\n",
    "noisecross = np.asarray(Image.open('images/noisecross.png'))\n",
    "_ = plt.imshow(noisecross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90b416-375e-4c68-882e-16004ce94754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate big and small footprints.\n",
    "# note that what's 'big' and 'small' depends on the\n",
    "# size of the image you're applying a filter to!\n",
    "fbig, fsmall = 10, 3\n",
    "big_foot, small_foot = np.ones((fbig, fbig)), np.ones((fsmall, fsmall))\n",
    "# these are just arrays:\n",
    "small_foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f55379-86e7-468b-86ba-c58ca2296397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is a convenience function for displaying multiple images\n",
    "# at once as rows or columns. I think we might want to move it out of\n",
    "# the notebook; it's a little cheaty but it's just SO MUCH matplotlib\n",
    "# boilerplate if we don't have it, and I don't know if we want\n",
    "# to explain it.\n",
    "def imshow_multiple(\n",
    "    *images, \n",
    "    direction=\"row\", \n",
    "    titles=None, \n",
    "    base_figsize=4,\n",
    "    **imshow_kwargs\n",
    "):\n",
    "    titles = [None for _ in images] if titles is None else titles\n",
    "    plotshape = (1, len(images)) if direction == \"row\" else (len(images), 1)\n",
    "    shapes = np.array([i.shape[:2] for i in images])\n",
    "    aspects = shapes[:, 0] / shapes[:, 1]\n",
    "    worst_aspect = aspects[np.argmax(np.abs(aspects - 1))]\n",
    "    figsize = (\n",
    "        base_figsize / worst_aspect * plotshape[1],\n",
    "        base_figsize * worst_aspect * plotshape[0]\n",
    "    )\n",
    "    fig, axes = plt.subplots(*plotshape, figsize=figsize)\n",
    "    axes = [axes] if len(images) == 1 else axes\n",
    "    for im, ax, title in zip(images, axes, titles):\n",
    "        ax.imshow(im, **imshow_kwargs)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2812eb6-1cc2-4efb-aa31-7ad35a62461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The median filter has a 'softening' effect. When the footprint is small, the \n",
    "# softening is relatively mild; as it gets bigger, the softening gets more intense.\n",
    "# This is a little bit like turning up the bass on a piece of music: it emphasizes\n",
    "# more consistent parts of the image and suppresses more variable ones.\n",
    "nc_small_median = ndi.median_filter(noisecross, footprint=small_foot)\n",
    "nc_big_median = ndi.median_filter(noisecross, footprint=big_foot)\n",
    "_ = imshow_multiple(\n",
    "    noisecross, \n",
    "    nc_small_median,\n",
    "    nc_big_median,\n",
    "    titles = (\"original\", \"median_small\", \"median_big\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efbb64-708b-482a-aba1-8d9c05517b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be used for many practical purposes. For instance, look \n",
    "# at what a median filter can do to the scanlines in this classic Viking Orbiter image\n",
    "# (a technique the ground team used to great advantage!):\n",
    "import pdr\n",
    "\n",
    "viking = pdr.read(\"/datascratch/viking/edr/vo_1023/f611axx/F611A13.IMG\").IMAGE\n",
    "_ = imshow_multiple(\n",
    "    viking, ndi.median_filter(viking, footprint=np.ones((4, 4))),\n",
    "    base_figsize=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4a36e-58bd-4888-bb28-e0ba5a077e82",
   "metadata": {},
   "source": [
    "## Morphological Operators\n",
    "\n",
    "Because subjective fullness of the Moon is basically about shape and line,\n",
    "we'd like to be able to work directly with those aspects of images. \n",
    "Morphological operators are powerful tools for doing this. Morphological\n",
    "operators are a type of footprint-based filter that use logical operations\n",
    "like \"and\" and \"or\" instead of arithmetic.\n",
    "\n",
    "### Making Binary Images\n",
    "\n",
    "Because they do true/false logic, morphological operators want to work on\n",
    "\"binary\" images -- black-and-white images made up of only 1s and 0s.\n",
    "\n",
    "Most images we want to work with don't start out as binary images. The\n",
    "easiest (and one of the most effective) ways to reduce images to lines\n",
    "is to set a cutoff value or \"threshold\". We then set all values below\n",
    "that threshold to black, and all values above that threshold to white. \n",
    "If it's a color image, we also want to turn it to grayscale first.\n",
    "This often works something like tracing or making an outline of an image --\n",
    "and these are also good first steps in other processes that want outlines,\n",
    "like silkscreening.\n",
    "\n",
    "Let's go ahead and walk through the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b6788-d657-429b-8783-38d88640a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a detail of part of a Tiffany lamp.\n",
    "tiffany = np.asarray(Image.open(\"images/tiffany.png\"))\n",
    "_ = plt.imshow(tiffany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d0668-7581-4f61-b95f-6e436ab2ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color images are usually 3-D arrays where the third axis\n",
    "# represents color channel, in this case red (R), green (G) or blue (B).\n",
    "# This would make a purple version of the image:\n",
    "purple_tiffany = tiffany.copy()\n",
    "purple_tiffany[:, :, 1] = 0\n",
    "_ = plt.imshow(purple_tiffany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd70672-d13b-4150-ba47-62332f2258b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easiest way to make a gray version of a color image is \n",
    "# just to merge its channels down, which can be as simple as\n",
    "# taking their mean or median:\n",
    "tiffany_gray = np.mean(tiffany, axis=2)\n",
    "_ = plt.imshow(tiffany_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041f3fe-dd5b-46e9-8434-72c89cc59d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now turn it to a black-and-white image with thresholding.\n",
    "# Picking the correct threshold value is a little bit of an art and\n",
    "# depends on exactly what features you want the outline to\n",
    "# retain. Let's see what happens at a few different levels...\n",
    "threshold_levels = (10, 25, 50, 160)\n",
    "outlines = [tiffany_gray > level for level in threshold_levels]\n",
    "imshow_multiple(*outlines, titles=threshold_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff68d2-1db1-4e03-9cd3-0fe903e61c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the second one for our outline -- but \n",
    "# you might to try the next steps a few times with different\n",
    "# settings to see what happens.\n",
    "tiffany_outline = tiffany_gray > 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56b709-b3d5-4fb1-ba11-b0e3b9ccfa3f",
   "metadata": {},
   "source": [
    "### Dilation and Erosion\n",
    "\n",
    "There are two basic morphological operators: erosion and dilation. Most\n",
    "other can be assembled from combinations of these two.\n",
    "\n",
    "Dilation is an \"or\". If there is a pixel valued 1 anywhere in the dilation \n",
    "operator's footprint, it sets the center pixel to 1; otherwise, it sets it\n",
    "to 0. Erosion is an \"and\": if _all_ pixels in the erosion operator's \n",
    "footprint are 1, it sets the center pixel to 1; otherwise, it sets the\n",
    "center pixel to 0.\n",
    "\n",
    "This means that erosion will tend to make black parts of the image heavier,\n",
    "thicker, and more coherent, and dilation will do the opposite.\n",
    "\n",
    "Let's look at erosion and dilation here. Note that, just like the median filter \n",
    "we saw earlier, bigger footprints tend to make morphological operatorsand selecting the correct footprint size\n",
    "or shape for particular images and applications can be a very finicky job. \"stronger\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d160aa-97a3-47b2-b5e0-fe6ee3bff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this particular image is basically a black-on-white outline, erosion \n",
    "# will tend to make its lines thicker and its sections more distinct:\n",
    "sizes = [4, 9]   \n",
    "eroded = [\n",
    "    ndi.binary_erosion(tiffany_outline, np.ones((fs, fs)))\n",
    "    for fs in sizes\n",
    "]\n",
    "_ = imshow_multiple(*eroded, titles=[f\"erosion {s}\" for s in sizes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf055b71-69d0-4f1a-abfe-13d28cc20c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas dilation will tend to make its lines thinner and blur sections:\n",
    "dilated = [\n",
    "    ndi.binary_dilation(tiffany_outline, np.ones((fs, fs)))\n",
    "    for fs in sizes\n",
    "]\n",
    "_ = imshow_multiple(*dilated, titles=[f\"dilation {s}\" for s in sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203393f-e9d6-4a96-838e-cb76fac4451e",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "\n",
    "Being able to manipulate lines like this is very useful in part because it\n",
    "lets us automatically segment images. The simplest way to do this is to\n",
    "simply find contiguous regions of the image and assign a unique number to \n",
    "them. This process is called \"labeling\", and many good tools for it exist\n",
    "in Python.\n",
    "\n",
    "In the next cell, you'll 'thicken' the Tiffany outline and then use `ndi.label()` \n",
    "to assign every contiguous 1-valued region of that outline its own unique number.\n",
    "It always assigns 0 to what it identifies as \"background\". \n",
    "You'll see that it doesn't work perfectly -- it may cut some sections at the \n",
    "edges off, and can't quite distinguish some regions that might look contiguous to \n",
    "your eye because of some junky little line bits that connect them.\n",
    "\n",
    "Like threshold levels, selecting good footprint sizes and shapes for particular images\n",
    "and applications can be a very finicky job. If you change the footprint size\n",
    "from (3, 3) and run the cell again, you will see that you get quite different labels.\n",
    "If you make it a lot bigger, you'll see that you get _fewer_ labels, because the erosion\n",
    "will have made image areas more distinct from one another...although if you make it big \n",
    "enough, you might not get any at all, because you'll have eroded all the lines into one\n",
    "big line. You'll also get very different results -- probably _much_ larger and more \n",
    "connected labels -- if you swap it to binary dilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafb7a1-d122-4933-88cb-f9b1aa1f6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = ndi.binary_erosion(tiffany_outline, np.ones((3, 3)))\n",
    "labels, n_labels = ndi.label(trace)\n",
    "print(f\"{n_labels} labels found.\")\n",
    "plt.imshow(np.ma.masked_where(labels == 0, labels), cmap='flag', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8c8fa-1c10-4796-9477-5472c7ceddcc",
   "metadata": {},
   "source": [
    "We can use this to pick out, count, and locate individual\n",
    "regions of an image. Try running this next cell a few times to drill\n",
    "down into the specific image elements our little algorithm identified.\n",
    "You'll note that not every label is interesting -- `ndi.label()`\n",
    "will happily assign a unique label to a tiny little dot, as long\n",
    "as it's isolated. When we look at lunar images a little later, \n",
    "we'll need to watch for that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6206fb1-366e-41b6-baa3-82176a38efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for visualization.\n",
    "\n",
    "def apply_2d_stencil(color_image, stencil_2d):\n",
    "    # reshape `stencil_2d` to fit the 3D color image\n",
    "    stencil_3d = np.moveaxis(np.tile(stencil_2d, (3, 1, 1)), 0, 2)\n",
    "    # return a 3D array that contains the original pixels where \n",
    "    # `stencil_2d` is truthy, and is black elsewhere\n",
    "    return np.where(stencil_3d, color_image, 0)\n",
    "\n",
    "def labelstencil(color_image, label_array, label_numbers):\n",
    "    \"\"\"\n",
    "    Return a copy of `color_image` blacked out wherever the values of \n",
    "    `label_array` don't fall within `label_numbers`.\n",
    "    \"\"\"\n",
    "    # make an array that's True for these labels and false otherwise\n",
    "    return apply_2d_stencil(color_image, np.isin(label_array, label_numbers))\n",
    "\n",
    "\n",
    "# select 8 random numbers from among all labels, not counting the \n",
    "# 0/background label.\n",
    "random_label_choices = np.random.choice(range(1, n_labels), 8)\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "ax.imshow(labelstencil(tiffany, labels, random_label_choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398c58d-adf6-4a73-991c-5d39421b155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also look at just the background:\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "ax.imshow(labelstencil(tiffany, labels, [0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8aaecc-8a22-4a54-a152-acd061fb3dc3",
   "metadata": {},
   "source": [
    "### Getting shapes from labels\n",
    "\n",
    "Because of image defects and the messy imperfection of the world,\n",
    "labels don't usually _exactly_ fit the regions you'd like to find\n",
    "in images, and they also often have very complex boundaries that \n",
    "make some kinds of analysis harder (even when the actual objects \n",
    "in the scene don't). One straightforward technique that's effective\n",
    "in many situations is simply to draw some simple geometric shape \n",
    "like a circle or rectangle around the label. (Sometimes you're not\n",
    "looking for simple geometric shapes, of course, in which case this\n",
    "would not be a good idea.)\n",
    "\n",
    "\n",
    "OpenCV has several fast, reliable functions for doing this.\n",
    "Its Python wrapper, `cv2`, is very effective, but its expectations \n",
    "are unusual and its error messages aren't always very readable (it's \n",
    "practically like a separate language). Rather than spend the rest of \n",
    "this Notebookjust talking about the OpenCV API, we've simply provided\n",
    "a couple of example functions in the next cell for performing common \n",
    "tasks of this kind with `cv2`.\n",
    "\n",
    "\n",
    "**NOTE: we could 'cheat' here as well and just have them import \n",
    "the `mask2shape` function. Actually explaining why you have to do all this \n",
    "array manipulation and type conversion to use even simple functions from \n",
    "the OpenCV API will take a while and be very distracting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69575916-934d-4b42-8ef4-50d3787a3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def mask2vec(arr):\n",
    "    \"\"\"\n",
    "    Converts the truthy elements of an ndarray into an array of \"vectors\"\n",
    "    suitable for use by OpenCV geometry functions.\n",
    "    \"\"\"\n",
    "    return np.vstack(np.nonzero(arr.T)).T\n",
    "\n",
    "def mask2shape(\n",
    "    mask, \n",
    "    shape = \"circle\", \n",
    "    color = (0, 255, 255), \n",
    "    thickness = 2,\n",
    "    draw_on_array = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws the smallest possible circle, triangle, or rectangle around the \n",
    "    truthy elements of an ndarray. Returns both the shape parameters and the \n",
    "    shape drawn in an ndarray. If draw_on_mask is True, draws the shape\n",
    "    on top of the existing elements of the mask.\n",
    "    \"\"\"\n",
    "    if shape not in {\"circle\", \"triangle\", \"rectangle\"}:\n",
    "        raise ValueError(\"Shape can be 'circle', 'triangle', or 'rectangle'.\")\n",
    "    vec, param = mask2vec(mask), {}\n",
    "    if draw_on_array is False:\n",
    "        canvas = np.zeros([*mask.shape, 3], 'u1')\n",
    "    else:\n",
    "        canvas = np.moveaxis(\n",
    "            np.tile(np.where(mask, 255, 0).astype('u1'), [3, 1, 1]), 2, 1\n",
    "        ).T.copy()\n",
    "    if shape == \"circle\":\n",
    "        (param['cx'], param['cy']), param['r'] = cv2.minEnclosingCircle(vec)\n",
    "        cx, cy, r = map(lambda p: int(round(p)), param.values())\n",
    "        return param, cv2.circle(canvas, (cx, cy), r, color, thickness)\n",
    "    if shape == \"triangle\":\n",
    "        param['area'], param['points'] = cv2.minEnclosingTriangle(vec)\n",
    "        return param, cv2.drawContours(\n",
    "            canvas, [param['points'].round().astype('i4')], 0, color, thickness\n",
    "        )\n",
    "    param['ul'], param['lr'], param['angle'] = cv2.minAreaRect(vec)\n",
    "    return param, cv2.drawContours(\n",
    "        canvas, \n",
    "        [cv2.boxPoints(tuple(param.values())).round().astype('i4')],\n",
    "        0,\n",
    "        color,\n",
    "        thickness\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604da924-4514-4a21-ab21-0ab604e28b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a simple example:\n",
    "random_label_choices = np.random.choice(range(1, n_labels), 8)\n",
    "assembled = np.zeros_like(tiffany)\n",
    "for label in random_label_choices:\n",
    "    _, drawn = mask2shape(labels == label, \"triangle\", thickness=7)\n",
    "    assembled += drawn\n",
    "fig, ax = plt.subplots(figsize=(5, 6))\n",
    "ax.imshow(\n",
    "    np.clip(assembled + labelstencil(tiffany, labels, random_label_choices), 0, 255)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69969b46-b856-41e4-88eb-aea72e296d6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SECTIONS SECTIONS SECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f92720-d906-46e2-ac73-97f62e7384a4",
   "metadata": {},
   "source": [
    "# Moon fullness identification algorithm\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "These are not completely true, but good enough for the algorithm:\n",
    "1. A full Moon looks like a perfect circle.\n",
    "2. The only things that ever appear in the GOES LUN images are stars, the Moon, the Earth, and static.\n",
    "3. The Earth will always appear as a contiguous region that touches at least one image edge.\n",
    "4. If the Moon touches an image edge, we can ignore it, because we won't be able to tell how full\n",
    "   it is anyway.\n",
    "5. The images are taken at about the same distance from the Moon, so the Moon will fall within a\n",
    "   small size range.\n",
    "6. There are no legitimate images of the Moon at less than half full in this set, so we can\n",
    "   always assume the Moon label is convex.\n",
    "\n",
    "## Steps\n",
    "\n",
    "Given those assumptions, we can perform the following steps to figure out how full\n",
    "the Moon looks in a GOES image:\n",
    "\n",
    "1. Load the image.\n",
    "2. Turn it into a black-and-white \"outline\" using thresholding.\n",
    "3. Apply an erosion operator to the outline to clean up static, stars, and imaging imperfections.\n",
    "4. Label the eroded outline, then exclude labels that are either the wrong size or too close to\n",
    "   an image edge. If we have exactly one label left after this, it's the Moon; proceed to step 5.\n",
    "   Otherwise, there's no Moon in the image; stop the algorithm.\n",
    "6. Apply a dilation operator to smooth the edges of the Moon label, and then \n",
    "7. Draw a circle around the label. The ratio of the area of that circle to the that ratio is\n",
    "   fullness.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "First, we'll build the algorithm step by step together -- but this time, we'll be asking _you_ to \n",
    "w\u0000e\u0000 appropriate parameters for the functions. Hints are available, but there's no strict\n",
    "right answer to this -- different sets of parameters might work equally well, and some might\n",
    "work better on some images and worse on others.\n",
    "\n",
    "Then, we'll try the algorithm on all the images and test it against \"ground truth\". If you don't\n",
    "like the results you get, then you can go back and tweak your parameters until you do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfff84-54b4-4e4b-9815-477a120ef22b",
   "metadata": {},
   "source": [
    "### Algorithm step 1: Load the image \n",
    "\n",
    "There's not much fancy to do here. The main thing to take note of is that\n",
    "the filters we're working with do _not_ like masked arrays, but NetCDF4 uses\n",
    "masks to represent bad values. This wouldn't work in every application, but\n",
    "in these images, we can get away with just setting every masked element to 0\n",
    "and throwing the mask away by using the `.filled(fill_value)` method of numpy\n",
    "masked arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3aece-cf68-4c7a-b1ba-4850a2c53445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "def load_goes_image(path):\n",
    "    array = Dataset(path).variables['radiance'][:]\n",
    "    return array.filled(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab02975-5e1b-40a9-85d0-0465053f13d5",
   "metadata": {},
   "source": [
    "### Pick images for pipeline tuning\n",
    "\n",
    "Go ahead and use this function to load in some test images so\n",
    "that you can tune the rest of the pipeline effectively.\n",
    "\n",
    "You probably want to pick at least one that contains a clear image \n",
    "of the Moon and one that doesn't. You could even just pick random\n",
    "ones until you see ones you like -- this is often a good idea for\n",
    "testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccd7d4-8286-460e-8b65-6276c75d4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "LUN_FOLDER = Path(\"/datascratch/goes_subset_flat/\")\n",
    "goes_lun_index = pd.read_csv(\"indices/lun_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133734c6-efb0-4e84-9040-99491ea01c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, n_images = [], 3\n",
    "for ix in random.choices(goes_lun_index.index, k=n_images):\n",
    "    path = LUN_FOLDER / goes_lun_index.loc[ix, 'name']\n",
    "    images.append(load_goes_image(path))\n",
    "_ = imshow_multiple(*images, base_figsize=9, direction=\"column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a3f2a-fcc4-4c17-9d36-c77e3dc40574",
   "metadata": {},
   "source": [
    "# Algorithm step 2: Make an outline of the image\n",
    "\n",
    "This is just like the Tiffany lamp, except these images are already\n",
    "grayscale, so it's _very_ simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0bf42-e8ca-49c2-9057-7abf19449592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outline(image, threshold):\n",
    "    return image > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e13-744f-4bce-9c73-3709b1320212",
   "metadata": {},
   "source": [
    "### Tune the threshold value\n",
    "\n",
    "Find a value for THRESHOLD that produces a sensible result on your\n",
    "test images. Too low and you'll get too much noise in the outline;\n",
    "too high and you won't get enough of the Moon (or not reliably, \n",
    "depending on exposure time).\n",
    "\n",
    "As a starting point, remember that the threshold value will want\n",
    "going to be between the `.min()` and `.max()` of the image,\n",
    "or it's going to just make the \"outline\" all white or all black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05511e68-8154-49e4-bdc5-0c1d9e2cdc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this next line and set it to some number, maybe\n",
    "# after a little extra data exploration:\n",
    "# THRESHOLD = 3\n",
    "\n",
    "outlines = [make_outline(i, THRESHOLD) for i in images]\n",
    "# this is a quick way to look at the effects of the function, but in this \n",
    "# step as well as later ones, you might also want to stop and examin3\n",
    "# individual images at larger sizes.\n",
    "_ = imshow_multiple(*outlines, direction=\"column\", base_figsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d2ab3-b138-4f5d-9dc1-48088d893847",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Algorithm step 3: Clean up the image\n",
    "\n",
    "You'll probably have at least some stars and noise and jaggy scanline stuff \n",
    "in your thresholded images, even if they're not immediately obvious (you might\n",
    "want to look at the images in a bigger form). These can make labeling slower \n",
    "and less reliable. This function simply applies a binary erosion operator to \n",
    "try to clean these up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c376c-e046-48cd-baaf-1d04910478e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode(image, erosion_size):\n",
    "    if erosion_size is None:\n",
    "        return image\n",
    "    return ndi.binary_erosion(image, np.ones((erosion_size, erosion_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51e398-5129-48ff-b4e4-1c699b7add5b",
   "metadata": {},
   "source": [
    "### Start assembling the handler function\n",
    "\n",
    "Now that we have two steps past the plain loading part, \n",
    "we'll want to roll them together so that we can run them all \n",
    "at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bee8a-cec1-4a2c-8fd1-1000ba38f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullness_algorithm(image, threshold, erosion_size):\n",
    "    outline = make_outline(image, threshold)\n",
    "    return erode(outline, erosion_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54bb5f-146b-4c6e-9084-f0841231376b",
   "metadata": {},
   "source": [
    "### Tune the erosion operator\n",
    "\n",
    "Find a good value for `EROSION_SIZE` that cleans some defects without getting\n",
    "rid of shapes you'd like to keep.\n",
    "\n",
    "It's often a good idea, when you're designing an algorithm like this, \n",
    "to allow yourself to try turning on and off individual steps that _might_ not \n",
    "be necessary (things like file loading obviously don't count), so `erode` has \n",
    "a provision for turning the step off if you'd like to try that: `EROSION_SIZE=None` \n",
    "and move on.\n",
    "\n",
    "Also keep in mind that all these parameters work together, so you may want to change\n",
    "your mind about what you set for THRESHOLD as you're working here.\n",
    "\n",
    "*NOTE: A possible alternative to this step, if you want to play around with the \n",
    "algorithm a little more, is to apply a median filter _before_ making the outline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720c956-84e8-46c7-90fd-cf4dd8ed15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1.1\n",
    "EROSION_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc353da-f58d-4722-80b0-568a10f21bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this next line and set it to your preferred size:\n",
    "# EROSION_SIZE = $some number\n",
    "\n",
    "processed = [\n",
    "    fullness_algorithm(\n",
    "        image, threshold=THRESHOLD, erosion_size=EROSION_SIZE\n",
    "    )\n",
    "    for image in images\n",
    "]\n",
    "# again, you might want to look at these at larger size as well.\n",
    "imshow_multiple(*processed, base_figsize=10, direction=\"column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5574f40-440a-4d81-bf95-3a23d01eabfb",
   "metadata": {},
   "source": [
    "### Algorithm step 4: Label the outline and filter out \"bad\" labels.\n",
    "\n",
    "Making a label array is easy at this point, but to apply our assumptions that the \n",
    "the Moon has a certain minimum extent within the image and that the Moon does not touch \n",
    "an image edge, we need to work with the _indices_ of the labels. Let's go ahead and define\n",
    "a little utility function that produces those along with the label array. (This \n",
    "definitely isn't the most efficient way to do it, but it makes it easy to inspect.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e07d3-fc6a-4acc-b765-b9c6add1fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexed_label(arr):\n",
    "    labels, n_labels = ndi.label(arr)\n",
    "    label_indices = {}\n",
    "    for l in range(n_labels + 1):  # we'd like to get 0 as well, just for fun\n",
    "        # np.nonzero() produces a tuple of ndarrays.\n",
    "        y_indices, x_indices = np.nonzero(labels == l)\n",
    "        label_indices[l] = {'y': y_indices, 'x': x_indices}\n",
    "    return labels, label_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d9e81-dc82-45d2-94aa-f5e500085a6c",
   "metadata": {},
   "source": [
    "Now we can iterate over all the labels and look at how big each one is and\n",
    "where it's positioned.\n",
    "\n",
    "*NOTE 1: This is effectively equivalent to measuring a bounding non-rotated \n",
    "rectangle for each label, but since we're only concerned with extent and \n",
    "position at this stage, we don't need to actually _make_ the rectangles.\n",
    "A more sophisticated version of this algorithm might, though.*\n",
    "\n",
    "*NOTE 2: We specifically don't want to construct circles here. One of the things\n",
    "we're doing by putting a lower limit on both width and height is expressing\n",
    "ideas like \"oh that could definitely not be a circle; it is way too long and thin.\"\n",
    "This is a good example of a step where you _shouldn't_ enforce the shapes you're looking\n",
    "for on labels -- they're not ready for that yet!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c992e29-88d5-4ca4-b547-f54320ad5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extent(indices):\n",
    "    return {\n",
    "        'area': len(indices['x']),  # same in each dimension\n",
    "        'width': np.ptp(indices['x']),  # just max value - min value\n",
    "        'height': np.ptp(indices['y'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44108347-093a-4d87-9be8-910dcd05010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88387d14-13f6-4452-9a08-c4945fe4fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab4778-fa27-423a-a1ea-1b5de699693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.meshgrid(np.arange(processed[1].shape[0]), np.arange(processed[1].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dba42-d43b-46a7-b9eb-0d308c9e43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, n_labels = ndi.label(processed[0])\n",
    "yix, xix = map(np.ravel, np.mgrid[0:labels.shape[0], 0:labels.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ba291-7d7e-48b2-9f62-8604c0dc9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = labels.ravel()\n",
    "for label in range(n_labels):\n",
    "    rix = np.nonzero(lr == label)\n",
    "    yi, xi = yix[rix], xix[rix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59824a78-a433-4411-a3ea-7eeebbebd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for label in range(n_labels):\n",
    "    yix, xix = np.nonzero(labels == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965921e-aaa3-452d-8435-3d47636f1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, inverse = np.unique(labels.ravel(), return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c109f6-bc5d-4090-bfd4-eb9e1ea101f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7335ab-1d18-48a9-b449-2bac801de196",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.nonzero(xix[inverse] != yix[inverse])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cbb20-8d3e-4a50-9383-0d288bf52242",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c785820-a2e7-4d9d-9059-f03ee27548ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f750a-bbf0-44e3-98dc-01ccaf1555db",
   "metadata": {},
   "outputs": [],
   "source": [
    "xix.ravel()[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86652af-ea13-4a61-ae6f-fac66977df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.ravel()[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524e5a7-d8aa-4f42-ae58-326b90b2f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0644302-3ab4-4bdf-b7eb-267007239e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xix[inverse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707f56c-a8fe-4e4d-b3dd-dad070b68ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af11506-371a-4469-b3a8-0186b9548266",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2559e8-7de8-44b3-b9f6-dee748ebf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yix.ravel()[inverse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46fcf8-3c9e-4b78-96de-621c97e97fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from typing import Union\n",
    "\n",
    "def a2c(area: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Area of a circle.\"\"\"\n",
    "    return 2 * np.pi * np.sqrt(area / np.pi)\n",
    "\n",
    "\n",
    "\n",
    "def filter_labels(\n",
    "    arr: np.ndarray, \n",
    "    labels: np.ndarray, \n",
    "    bordersize: int, \n",
    "    min_moon_extent: int\n",
    ") -> tuple[dict, dict, dict]:\n",
    "    \"\"\"\n",
    "    Filter 'bad' labels: labels that are too small or that come \n",
    "    within a specified range of the top/bottom/left/right of the image.\n",
    "    \"\"\"\n",
    "    label_indices = {\n",
    "        label: np.nonzero(labels == label)\n",
    "        for label in np.unique(labels)\n",
    "        if label != 0\n",
    "    }\n",
    "    label_rejects = {}\n",
    "    for label, (yix, xix) in label_indices.items():\n",
    "        if min([np.ptp(xix), np.ptp(yix)]) < min_moon_extent:\n",
    "            label_rejects[label] = 'small'\n",
    "        elif (xix <= bordersize).any():\n",
    "            label_rejects[label] = 'left'\n",
    "        elif (arr.shape[1] - xix <= bordersize).any():\n",
    "            label_rejects[label] = 'right'\n",
    "        elif (yix <= bordersize).any():\n",
    "            label_rejects[label] = 'top'\n",
    "        elif (arr.shape[0] - yix <= bordersize).any():\n",
    "            label_rejects[label] = 'bottom'\n",
    "    remaining = {\n",
    "        k: v for k, v in label_indices.items()\n",
    "        if k not in label_rejects.keys()\n",
    "    }\n",
    "    return remaining, label_rejects, label_indices\n",
    "\n",
    "\n",
    "def moon_extraction_error(rec: dict, n_remaining: int) -> dict:\n",
    "    if n_remaining == 0:\n",
    "        return rec | {'error': 'no Moon', 'moon': None}\n",
    "    if n_remaining > 1:\n",
    "        return rec | {'error': 'ambiguous Moon', 'moon': None}\n",
    "    raise ValueError(\"How did we get here?\")\n",
    "\n",
    "\n",
    "def get_moon_cutout(remaining: dict, moonpad: int) -> list[slice]:\n",
    "    moon_y, moon_x = next(iter(remaining.values()))\n",
    "    y0, y1, x0, x1 = moon_y.min(), moon_y.max(), moon_x.min(), moon_x.max()\n",
    "    return [\n",
    "        slice(max(y0 - moonpad, 0), y1 + moonpad),\n",
    "        slice(max(x0 - moonpad, 0), x1 + moonpad)\n",
    "    ]\n",
    "\n",
    "\n",
    "def mooncircle(labels, moonslice, moonlabel) -> dict:\n",
    "    \"\"\"\n",
    "    Given a dict containing a label array (keyed \"labels\") and a tuple of\n",
    "    Y/X slices giving the bounds of a detected \"Moon\" (keyed \"moonlabel\"),\n",
    "    compute the convex hull of the detected Moon, and approximate its\n",
    "    illuminated fraction by comparing the area of that hull to the area of its\n",
    "    minimum enclosing circle (which represents an implied full Moon).\n",
    "    \"\"\"\n",
    "    moonblob = labels[*moonslice] == moonlabel\n",
    "    smoothblob = ndi.binary_dilation(moonblob, np.ones((5, 5))).astype('u1')\n",
    "    hull = gethull(smoothblob)\n",
    "    center, radius = cv2.minEnclosingCircle(hull)\n",
    "    return {\n",
    "        'smoothblob': smoothblob,\n",
    "        'center': center,\n",
    "        'radius': radius,\n",
    "        'ratio': cv2.contourArea(hull) / (np.pi * radius ** 2)\n",
    "    }\n",
    "    \n",
    "\n",
    "# TODO: don't magically provide good parameters. make them mess with it.\n",
    "def extract_moon(\n",
    "    arr: np.ndarray,\n",
    "    threshold: float = 0.7,\n",
    "    erosion: int = 4,\n",
    "    bordersize: int = 50,\n",
    "    moonpad: int = 60,\n",
    "    min_moon_extent: int = 180,\n",
    "    extended: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Attempt to identify/locate a Moon in a 2D array taken from a GOES LUN\n",
    "    radiance image. Takes a number of parameters for different steps of the\n",
    "    extraction pipeline:\n",
    "    * threshold: cutoff value for making initial threshold array\n",
    "    * erosion: side length of square footprint for erosion operator applied\n",
    "      to threshold array\n",
    "    * bordersize: image border width for top/bottom/left/right label rejection\n",
    "    * moonpad: number of \"padding\" pixels to add to the Moon cutout\n",
    "    * min_moon_extent: minimum extent, along both X and Y axes, for identifying\n",
    "      a label as the Moon\n",
    "    * extended: return extended information?\n",
    "\n",
    "    TODO: internal documentation, split out, explain, blah blah\n",
    "    \"\"\"\n",
    "    morph = (arr.filled(0) > threshold)\n",
    "    if erosion is not None:\n",
    "        morph = ndi.binary_erosion(morph, np.ones((erosion, erosion)))\n",
    "    morph = morph.astype('u1')\n",
    "    labels, n_labels = ndi.label(morph)\n",
    "    remaining, label_rejects, label_indices = filter_labels(\n",
    "        arr, labels, bordersize, min_moon_extent\n",
    "    )\n",
    "    rec = {'n_labels': n_labels, 'reject_reasons': label_rejects}\n",
    "    if extended is True:\n",
    "        rec |= {'labels': labels, 'label_indices': label_indices}\n",
    "    if len(remaining) != 1:\n",
    "        return moon_extraction_error(rec, len(remaining))\n",
    "    moonlabel = next(iter(remaining.keys()))\n",
    "    moonslice = get_moon_cutout(remaining, moonpad)\n",
    "    rec |= mooncircle(labels, moonslice, moonlabel)\n",
    "    if extended is True:\n",
    "        rec['moonlabel'] = moonlabel\n",
    "        rec['moonslice'] = moonslice\n",
    "    return rec | {'error': None, 'moon': arr[*moonslice]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cb25b-081b-4f1b-bca6-667d49d33718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
