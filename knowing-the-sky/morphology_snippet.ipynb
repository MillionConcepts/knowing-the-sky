{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446fef1e-23c0-482b-add2-e5a235d07f4b",
   "metadata": {},
   "source": [
    "## Introduction to Low-Level Image Processing\n",
    "\n",
    "You can get a long way in image processing and computer vision with\n",
    "fairly simple operations that basically just work with arithmetic.\n",
    "They're not only easier to use than more complex algorithms, but \n",
    "also usually faster, easier to inspect, and more reliable than \n",
    "complex algorithms. \n",
    "\n",
    "It's good to understand low-level techniques if you think you might want \n",
    "to use more complex techniques for image recognition or processing, \n",
    "including machine learning methods, because complex techniques often \n",
    "don't work well without preprocessing by simpler functions. Simple \n",
    "functions can also be assembled into quite complex ones!\n",
    "\n",
    "In those notebook, we'll primarily be using \"morphological\" operations \n",
    "-- operations that work with line, shape, and form. However, we'll start\n",
    "with a little introduction to their extended family.\n",
    "\n",
    "### Footprint Operations\n",
    "\n",
    "Many image manipulation techniques rely on looking at each pixel in the \n",
    "image, then applying some kind of mathematical function to the pixels \n",
    "that fall within some region around it. This region is called a \"footprint\"\n",
    "(or sometimes a \"kernel\").  \n",
    "\n",
    "Let's look briefly at a useful _non_-morphological example of this\n",
    "technique: the median filter. This filter works by replacing each pixel \n",
    "in an image with the median value of the pixels that fall within a footprint\n",
    "around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b075d-d059-42d5-b9c4-8e4b999f5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# NOTE: I don't remember if we opened 'desktop' images before.\n",
    "from PIL import Image\n",
    "import scipy.ndimage as ndi\n",
    "# This is a little settings file that removes all the annoying\n",
    "# borders and axes and things that aren't really relevant\n",
    "# when we're just using matplotlib to look at things as images.\n",
    "plt.style.use('settings/simple_image.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587859cb-6122-4141-9944-47fbc72b8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a noisy cross shape.\n",
    "noisecross = np.asarray(Image.open('images/noisecross.png'))\n",
    "_ = plt.imshow(noisecross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90b416-375e-4c68-882e-16004ce94754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate big and small footprints.\n",
    "# note that what's 'big' and 'small' depends on the\n",
    "# size of the image you're applying a filter to!\n",
    "fbig, fsmall = 10, 3\n",
    "big_foot, small_foot = np.ones((fbig, fbig)), np.ones((fsmall, fsmall))\n",
    "# these are just arrays:\n",
    "small_foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2812eb6-1cc2-4efb-aa31-7ad35a62461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The median filter has a 'softening' effect. When the footprint is small, the \n",
    "# softening is relatively mild; as it gets bigger, the softening gets more intense.\n",
    "# This is a little bit like turning up the bass on a piece of music: it emphasizes\n",
    "# more consistent parts of the image and suppresses more variable ones.\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].imshow(noisecross)\n",
    "axes[1].imshow(ndi.median_filter(noisecross, footprint=small_foot))\n",
    "axes[2].imshow(ndi.median_filter(noisecross, footprint=big_foot))\n",
    "for i, title in zip((0, 1, 2), (\"original\", \"median_small\", \"median_big\")):\n",
    "    axes[i].set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efbb64-708b-482a-aba1-8d9c05517b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be used for many practical purposes. For instance, look \n",
    "# at what a median filter can do to the scanlines in this classic Viking Orbiter image\n",
    "# (a technique the ground team used to great advantage!):\n",
    "import pdr\n",
    "viking = pdr.read(\"/datascratch/viking/edr/vo_1023/f611axx/F611A13.IMG\").IMAGE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 9))\n",
    "axes[0].imshow(viking)\n",
    "axes[1].imshow(ndi.median_filter(viking, footprint=np.ones((4, 4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4a36e-58bd-4888-bb28-e0ba5a077e82",
   "metadata": {},
   "source": [
    "## Morphological Operators\n",
    "\n",
    "Because subjective fullness of the Moon is basically about shape and line,\n",
    "we'd like to be able to work directly with those aspects of images. \n",
    "Morphological operators are powerful tools for doing this. Morphological\n",
    "operators are a type of footprint-based filter that use logical operations\n",
    "like \"and\" and \"or\" instead of arithmetic.\n",
    "\n",
    "### Making Binary Images\n",
    "\n",
    "Because they do true/false logic, morphological operators want to work on\n",
    "\"binary\" images -- black-and-white images made up of only 1s and 0s.\n",
    "\n",
    "Most images we want to work with don't start out as binary images. The\n",
    "easiest (and one of the most effective) ways to reduce images to lines\n",
    "is to set a cutoff value or \"threshold\". We then set all values below\n",
    "that threshold to black, and all values above that threshold to white. \n",
    "If it's a color image, we also want to turn it to grayscale first.\n",
    "This often works something like tracing or making an outline of an image --\n",
    "and these are also good first steps in other processes that want outlines,\n",
    "like silkscreening.\n",
    "\n",
    "Let's go ahead and walk through the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b6788-d657-429b-8783-38d88640a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a detail of part of a Tiffany lamp.\n",
    "tiffany = np.asarray(Image.open(\"images/tiffany.png\"))\n",
    "plt.imshow(tiffany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d0668-7581-4f61-b95f-6e436ab2ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color images are usually 3-D arrays where the third axis\n",
    "# represents color channel, in this case red (R), green (G) or blue (B).\n",
    "# This would make a purple version of the image:\n",
    "purple_tiffany = tiffany.copy()\n",
    "purple_tiffany[:, :, 1] = 0\n",
    "plt.imshow(purple_tiffany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e87772-6f52-434a-9e2f-ee12dd3b1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marslab.imgops.imgutils import eightbit, enhance_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2881e85-0e2c-41a2-a072-c0bcc5e7f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(enhance_color(tiffany, (0, 1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd70672-d13b-4150-ba47-62332f2258b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easiest way to make a gray version of a color image is \n",
    "# just to merge its channels down, which can be as simple as\n",
    "# taking their mean or median:\n",
    "tiffany_gray = np.mean(tiffany, axis=2)\n",
    "plt.imshow(tiffany_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041f3fe-dd5b-46e9-8434-72c89cc59d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now turn it to a black-and-white image with thresholding.\n",
    "# Picking the correct threshold value is a little bit of an art and\n",
    "# depends on exactly what features you want the outline to\n",
    "# retain. Let's see what happens at a few different levels...\n",
    "fig, axes = plt.subplots(1, 4, figsize=(13, 8))\n",
    "threshold_levels = (10, 25, 50, 160)\n",
    "for i, level in enumerate(threshold_levels):\n",
    "    axes[i].imshow(tiffany_gray > level)\n",
    "    axes[i].set_title(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff68d2-1db1-4e03-9cd3-0fe903e61c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the second one for our outline -- but \n",
    "# you might to try the next steps a few times with different\n",
    "# settings to see what happens.\n",
    "tiffany_outline = tiffany_gray > 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56b709-b3d5-4fb1-ba11-b0e3b9ccfa3f",
   "metadata": {},
   "source": [
    "### Dilation and Erosion\n",
    "\n",
    "There are two basic morphological operators: erosion and dilation. Most\n",
    "other can be assembled from combinations of these two.\n",
    "\n",
    "Dilation is an \"or\". If there is a pixel valued 1 anywhere in the dilation \n",
    "operator's footprint, it sets the center pixel to 1; otherwise, it sets it\n",
    "to 0. Erosion is an \"and\": if _all_ pixels in the erosion operator's \n",
    "footprint are 1, it sets the center pixel to 1; otherwise, it sets the\n",
    "center pixel to 0.\n",
    "\n",
    "This means that erosion will tend to make black parts of the image heavier,\n",
    "thicker, and more coherent, and dilation will do the opposite.\n",
    "\n",
    "Let's look at erosion and dilation here. Note that, just like the median filter \n",
    "we saw earlier, bigger footprints tend to make morphological operatorsand selecting the correct footprint size\n",
    "or shape for particular images and applications can be a very finicky job. \"stronger\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d160aa-97a3-47b2-b5e0-fe6ee3bff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this particular image is basically a black-on-white outline, erosion \n",
    "# will tend to make its lines thicker and its sections more distinct:\n",
    "small, big = 4, 9\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 8))\n",
    "for i, size in enumerate((small, big)):\n",
    "    footprint = np.ones((size, size))\n",
    "    axes[i].imshow(ndi.binary_erosion(tiffany_outline, footprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf055b71-69d0-4f1a-abfe-13d28cc20c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas dilation will tend to make its lines thinner and blur sections:\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 8))\n",
    "for i, size in enumerate((small, big)):\n",
    "    footprint = np.ones((size, size))\n",
    "    axes[i].imshow(ndi.binary_dilation(tiffany_outline, footprint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203393f-e9d6-4a96-838e-cb76fac4451e",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "\n",
    "Being able to manipulate lines like this is very useful in part because it\n",
    "lets us automatically segment images. The simplest way to do this is to\n",
    "simply find contiguous regions of the image and assign a unique number to \n",
    "them. This process is called \"labeling\", and many good tools for it exist\n",
    "in Python.\n",
    "\n",
    "In the next cell, you'll 'thicken' the Tiffany outline and then use `ndi.label()` \n",
    "to assign every contiguous 1-valued region of that outline its own unique number.\n",
    "It always assigns 0 to what it identifies as \"background\". \n",
    "You'll see that it doesn't work perfectly -- it may cut some sections at the \n",
    "edges off, and can't quite distinguish some regions that might look contiguous to \n",
    "your eye because of some junky little line bits that connect them.\n",
    "\n",
    "Like threshold levels, selecting good footprint sizes and shapes for particular images\n",
    "and applications can be a very finicky job. If you change the footprint size\n",
    "from (3, 3) and run the cell again, you will see that you get quite different labels.\n",
    "If you make it a lot bigger, you'll see that you get _fewer_ labels, because the erosion\n",
    "will have made image areas more distinct from one another...although if you make it big \n",
    "enough, you might not get any at all, because you'll have eroded all the lines into one\n",
    "big line. You'll also get very different results -- probably _much_ larger and more \n",
    "connected labels -- if you swap it to binary dilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafb7a1-d122-4933-88cb-f9b1aa1f6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = ndi.binary_erosion(tiffany_outline, np.ones((3, 3)))\n",
    "labels, n_labels = ndi.label(trace)\n",
    "print(f\"{n_labels} labels found.\")\n",
    "plt.imshow(np.ma.masked_where(labels == 0, labels), cmap='flag', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8c8fa-1c10-4796-9477-5472c7ceddcc",
   "metadata": {},
   "source": [
    "We can use this to pick out, count, and locate individual\n",
    "regions of an image. Try running this next cell a few times to drill\n",
    "down into the specific image elements our little algorithm identified.\n",
    "You'll note that not every label is interesting -- `ndi.label()`\n",
    "will happily assign a unique label to a tiny little dot, as long\n",
    "as it's isolated. When we look at lunar images a little later, \n",
    "we'll need to watch for that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6206fb1-366e-41b6-baa3-82176a38efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for visualization.\n",
    "\n",
    "def apply_2d_stencil(color_image, stencil_2d):\n",
    "    # reshape `stencil_2d` to fit the 3D color image\n",
    "    stencil_3d = np.moveaxis(np.tile(stencil_2d, (3, 1, 1)), 0, 2)\n",
    "    # return a 3D array that contains the original pixels where \n",
    "    # `stencil_2d` is truthy, and is black elsewhere\n",
    "    return np.where(stencil_3d, color_image, 0)\n",
    "\n",
    "def labelstencil(color_image, label_array, label_numbers):\n",
    "    \"\"\"\n",
    "    Return a copy of `color_image` blacked out wherever the values of \n",
    "    `label_array` don't fall within `label_numbers`.\n",
    "    \"\"\"\n",
    "    # make an array that's True for these labels and false otherwise\n",
    "    return apply_2d_stencil(color_image, np.isin(label_array, label_numbers))\n",
    "\n",
    "\n",
    "# select 8 random numbers from among all labels, not counting the \n",
    "# 0/background label.\n",
    "random_label_choices = np.random.choice(range(1, n_labels), 8)\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "ax.imshow(labelstencil(tiffany, labels, random_label_choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398c58d-adf6-4a73-991c-5d39421b155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also look at just the background:\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "ax.imshow(labelstencil(tiffany, labels, [0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8aaecc-8a22-4a54-a152-acd061fb3dc3",
   "metadata": {},
   "source": [
    "### Getting shapes from labels\n",
    "\n",
    "Because of image defects and the messy imperfection of the world,\n",
    "labels don't usually _exactly_ fit the regions you'd like to find\n",
    "in images, and they also often have very complex boundaries that \n",
    "make some kinds of analysis harder (even when the actual objects \n",
    "in the scene don't). One straightforward technique that's effective\n",
    "in many situations is simply to draw some simple geometric shape \n",
    "like a circle or rectangle around the label. (Sometimes you're not\n",
    "looking for simple geometric shapes, of course, in which case this\n",
    "would not be a good idea.)\n",
    "\n",
    "\n",
    "OpenCV has several fast, reliable functions for doing this.\n",
    "Its Python wrapper, `cv2`, is very effective, but its expectations \n",
    "are unusual and its error messages aren't always very readable (it's \n",
    "practically like a separate language). Rather than spend the rest of \n",
    "this Notebookjust talking about the OpenCV API, we've simply provided\n",
    "a couple of example functions in the next cell for performing common \n",
    "tasks of this kind with `cv2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69575916-934d-4b42-8ef4-50d3787a3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def mask2vec(arr):\n",
    "    \"\"\"\n",
    "    Converts the truthy elements of an ndarray into an array of \"vectors\"\n",
    "    suitable for use by OpenCV geometry functions.\n",
    "    \"\"\"\n",
    "    return np.vstack(np.nonzero(arr.T)).T\n",
    "\n",
    "def mask2shape(\n",
    "    mask, \n",
    "    shape = \"circle\", \n",
    "    color = (0, 255, 255), \n",
    "    thickness = 2,\n",
    "    draw_on_array = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws the smallest possible circle, triangle, or rectangle around the \n",
    "    truthy elements of an ndarray. Returns both the shape parameters and the \n",
    "    shape drawn in an ndarray. If draw_on_mask is True, draws the shape\n",
    "    on top of the existing elements of the mask.\n",
    "    \"\"\"\n",
    "    if shape not in {\"circle\", \"triangle\", \"rectangle\"}:\n",
    "        raise ValueError(\"Shape can be 'circle', 'triangle', or 'rectangle'.\")\n",
    "    vec, param = mask2vec(mask), {}\n",
    "    if draw_on_array is False:\n",
    "        canvas = np.zeros([*mask.shape, 3], 'u1')\n",
    "    else:\n",
    "        canvas = np.moveaxis(\n",
    "            np.tile(np.where(mask, 255, 0).astype('u1'), [3, 1, 1]), 2, 1\n",
    "        ).T.copy()\n",
    "    if shape == \"circle\":\n",
    "        (param['cx'], param['cy']), param['r'] = cv2.minEnclosingCircle(vec)\n",
    "        cx, cy, r = map(lambda p: int(round(p)), param.values())\n",
    "        return param, cv2.circle(canvas, (cx, cy), r, color, thickness)\n",
    "    if shape == \"triangle\":\n",
    "        param['area'], param['points'] = cv2.minEnclosingTriangle(vec)\n",
    "        return param, cv2.drawContours(\n",
    "            canvas, [param['points'].round().astype('i4')], 0, color, thickness\n",
    "        )\n",
    "    param['ul'], param['lr'], param['angle'] = cv2.minAreaRect(vec)\n",
    "    return param, cv2.drawContours(\n",
    "        canvas, \n",
    "        [cv2.boxPoints(tuple(param.values())).round().astype('i4')],\n",
    "        0,\n",
    "        color,\n",
    "        thickness\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604da924-4514-4a21-ab21-0ab604e28b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a simple example:\n",
    "random_label_choices = np.random.choice(range(1, n_labels), 8)\n",
    "assembled = np.zeros_like(tiffany)\n",
    "for label in random_label_choices:\n",
    "    _, drawn = mask2shape(labels == label, \"triangle\", thickness=7)\n",
    "    assembled += drawn\n",
    "fig, ax = plt.subplots(figsize=(5, 6))\n",
    "ax.imshow(\n",
    "    np.clip(assembled + labelstencil(tiffany, labels, random_label_choices), 0, 255)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69969b46-b856-41e4-88eb-aea72e296d6f",
   "metadata": {},
   "source": [
    "# SECTIONS SECTIONS SECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f92720-d906-46e2-ac73-97f62e7384a4",
   "metadata": {},
   "source": [
    "# Moon fullness identification algorithm\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "These are not completely true, but good enough for the algorithm:\n",
    "1. A full Moon looks like a perfect circle.\n",
    "2. The only things that ever appear in the GOES LUN images are stars, the Moon, the Earth, and static.\n",
    "3. The Earth will always appear as a contiguous region that touches at least one image edge.\n",
    "4. If the Moon touches an image edge, we can ignore it, because we won't be able to tell how full\n",
    "   it is anyway.\n",
    "5. The images are taken at about the same distance from the Moon, so the Moon will fall within a\n",
    "   small size range.\n",
    "6. There are no legitimate images of the Moon at less than half full in this set, so we can\n",
    "   always assume the Moon label is convex.\n",
    "\n",
    "## Steps\n",
    "\n",
    "Given those assumptions, we can perform the following steps to figure out how full\n",
    "the Moon looks in a GOES image:\n",
    "\n",
    "1. Turn the image into an outline using thresholding.\n",
    "2. Apply an erosion operator to the outline to clean up static, stars, and imaging imperfections.\n",
    "3. Label the eroded outline.\n",
    "4. Exclude labels that are:\n",
    "    1. the wrong size\n",
    "    2. too close to an image edge\n",
    "5. If we have exactly one label left, it's the Moon; proceed to step 6. Otherwise, there's no Moon in the image.\n",
    "6. Apply a dilation operator to smooth the edges of the Moon label, and then \n",
    "7. Draw a circle around the label. The ratio of the area of that circle to the that ratio is\n",
    "   fullness.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "First, we'll build the algorithm step by step together -- but this time, we'll be asking _you_ to \n",
    "w\u0000e\u0000 appropriate parameters for the functions. Hints are available, but there's no strict\n",
    "right answer to this -- different sets of parameters might work equally well, and some might\n",
    "work better on some images and worse on others.\n",
    "\n",
    "Then, we'll try the algorithm on all the images and test it against \"ground truth\". If you don't\n",
    "like the results you get, then you can go back and tweak your parameters until you do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46fcf8-3c9e-4b78-96de-621c97e97fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from typing import Union\n",
    "\n",
    "def a2c(area: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Area of a circle.\"\"\"\n",
    "    return 2 * np.pi * np.sqrt(area / np.pi)\n",
    "\n",
    "\n",
    "\n",
    "def filter_labels(\n",
    "    arr: np.ndarray, \n",
    "    labels: np.ndarray, \n",
    "    bordersize: int, \n",
    "    min_moon_extent: int\n",
    ") -> tuple[dict, dict, dict]:\n",
    "    \"\"\"\n",
    "    Filter 'bad' labels: labels that are too small or that come \n",
    "    within a specified range of the top/bottom/left/right of the image.\n",
    "    \"\"\"\n",
    "    label_indices = {\n",
    "        label: np.nonzero(labels == label)\n",
    "        for label in np.unique(labels)\n",
    "        if label != 0\n",
    "    }\n",
    "    label_rejects = {}\n",
    "    for label, (yix, xix) in label_indices.items():\n",
    "        if min([np.ptp(xix), np.ptp(yix)]) < min_moon_extent:\n",
    "            label_rejects[label] = 'small'\n",
    "        elif (xix <= bordersize).any():\n",
    "            label_rejects[label] = 'left'\n",
    "        elif (arr.shape[1] - xix <= bordersize).any():\n",
    "            label_rejects[label] = 'right'\n",
    "        elif (yix <= bordersize).any():\n",
    "            label_rejects[label] = 'top'\n",
    "        elif (arr.shape[0] - yix <= bordersize).any():\n",
    "            label_rejects[label] = 'bottom'\n",
    "    remaining = {\n",
    "        k: v for k, v in label_indices.items()\n",
    "        if k not in label_rejects.keys()\n",
    "    }\n",
    "    return remaining, label_rejects, label_indices\n",
    "\n",
    "\n",
    "def moon_extraction_error(rec: dict, n_remaining: int) -> dict:\n",
    "    if n_remaining == 0:\n",
    "        return rec | {'error': 'no Moon', 'moon': None}\n",
    "    if n_remaining > 1:\n",
    "        return rec | {'error': 'ambiguous Moon', 'moon': None}\n",
    "    raise ValueError(\"How did we get here?\")\n",
    "\n",
    "\n",
    "def get_moon_cutout(remaining: dict, moonpad: int) -> list[slice]:\n",
    "    moon_y, moon_x = next(iter(remaining.values()))\n",
    "    y0, y1, x0, x1 = moon_y.min(), moon_y.max(), moon_x.min(), moon_x.max()\n",
    "    return [\n",
    "        slice(max(y0 - moonpad, 0), y1 + moonpad),\n",
    "        slice(max(x0 - moonpad, 0), x1 + moonpad)\n",
    "    ]\n",
    "\n",
    "\n",
    "def mooncircle(labels, moonslice, moonlabel) -> dict:\n",
    "    \"\"\"\n",
    "    Given a dict containing a label array (keyed \"labels\") and a tuple of\n",
    "    Y/X slices giving the bounds of a detected \"Moon\" (keyed \"moonlabel\"),\n",
    "    compute the convex hull of the detected Moon, and approximate its\n",
    "    illuminated fraction by comparing the area of that hull to the area of its\n",
    "    minimum enclosing circle (which represents an implied full Moon).\n",
    "    \"\"\"\n",
    "    moonblob = labels[*moonslice] == moonlabel\n",
    "    smoothblob = ndi.binary_dilation(moonblob, np.ones((5, 5))).astype('u1')\n",
    "    hull = gethull(smoothblob)\n",
    "    center, radius = cv2.minEnclosingCircle(hull)\n",
    "    return {\n",
    "        'smoothblob': smoothblob,\n",
    "        'center': center,\n",
    "        'radius': radius,\n",
    "        'ratio': cv2.contourArea(hull) / (np.pi * radius ** 2)\n",
    "    }\n",
    "    \n",
    "\n",
    "# TODO: don't magically provide good parameters. make them mess with it.\n",
    "def extract_moon(\n",
    "    arr: np.ndarray,\n",
    "    threshold: float = 0.7,\n",
    "    erosion: int = 4,\n",
    "    bordersize: int = 50,\n",
    "    moonpad: int = 60,\n",
    "    min_moon_extent: int = 180,\n",
    "    extended: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Attempt to identify/locate a Moon in a 2D array taken from a GOES LUN\n",
    "    radiance image. Takes a number of parameters for different steps of the\n",
    "    extraction pipeline:\n",
    "    * threshold: cutoff value for making initial threshold array\n",
    "    * erosion: side length of square footprint for erosion operator applied\n",
    "      to threshold array\n",
    "    * bordersize: image border width for top/bottom/left/right label rejection\n",
    "    * moonpad: number of \"padding\" pixels to add to the Moon cutout\n",
    "    * min_moon_extent: minimum extent, along both X and Y axes, for identifying\n",
    "      a label as the Moon\n",
    "    * extended: return extended information?\n",
    "\n",
    "    TODO: internal documentation, split out, explain, blah blah\n",
    "    \"\"\"\n",
    "    morph = (arr.filled(0) > threshold)\n",
    "    if erosion is not None:\n",
    "        morph = ndi.binary_erosion(morph, np.ones((erosion, erosion)))\n",
    "    morph = morph.astype('u1')\n",
    "    labels, n_labels = ndi.label(morph)\n",
    "    remaining, label_rejects, label_indices = filter_labels(\n",
    "        arr, labels, bordersize, min_moon_extent\n",
    "    )\n",
    "    rec = {'n_labels': n_labels, 'reject_reasons': label_rejects}\n",
    "    if extended is True:\n",
    "        rec |= {'labels': labels, 'label_indices': label_indices}\n",
    "    if len(remaining) != 1:\n",
    "        return moon_extraction_error(rec, len(remaining))\n",
    "    moonlabel = next(iter(remaining.keys()))\n",
    "    moonslice = get_moon_cutout(remaining, moonpad)\n",
    "    rec |= mooncircle(labels, moonslice, moonlabel)\n",
    "    if extended is True:\n",
    "        rec['moonlabel'] = moonlabel\n",
    "        rec['moonslice'] = moonslice\n",
    "    return rec | {'error': None, 'moon': arr[*moonslice]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cb25b-081b-4f1b-bca6-667d49d33718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
